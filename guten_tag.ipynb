{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47bada6",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Hearlvein/colab/blob/main/guten_tag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e78fb2",
   "metadata": {
    "id": "85e78fb2"
   },
   "source": [
    "# ðŸª Fine-Tuning GPT-2 with LoRA for Poetic Sci-Fi/Fantasy Story Generation\n",
    "\n",
    "This notebook guides you through fine-tuning GPT-2 using Low-Rank Adaptation (LoRA) to generate poetic sci-fi/fantasy short stories. The process includes:\n",
    "- Dataset preparation from Project Gutenberg\n",
    "- Model fine-tuning with LoRA\n",
    "- Story generation based on user prompts\n",
    "\n",
    "**Note:** This notebook is designed for execution in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb754d",
   "metadata": {
    "id": "38cb754d"
   },
   "source": [
    "## ðŸ”§ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc4a906",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dc4a906",
    "outputId": "42d8fc6c-42f1-4ef2-ade6-e9e485bf3274"
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install -q transformers datasets peft trl bitsandbytes accelerate\n",
    "!pip install -q beautifulsoup4 requests gutenbergpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b944d2",
   "metadata": {
    "id": "05b944d2"
   },
   "source": [
    "## ðŸ“š Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae02ca8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae02ca8a",
    "outputId": "eae0a526-346c-4b8d-87e3-9a825a6ddeba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gutenberg metadata cache...\n",
      "Downloading book ID 145...\n",
      "Saved book 145 to gutenberg_dataset/science_fiction/145.txt\n",
      "Downloading book ID 2160...\n",
      "Saved book 2160 to gutenberg_dataset/science_fiction/2160.txt\n",
      "Downloading book ID 1259...\n",
      "Saved book 1259 to gutenberg_dataset/science_fiction/1259.txt\n",
      "Downloading book ID 4085...\n",
      "Saved book 4085 to gutenberg_dataset/science_fiction/4085.txt\n",
      "Downloading book ID 98...\n",
      "Saved book 98 to gutenberg_dataset/science_fiction/98.txt\n",
      "Downloading book ID 2600...\n",
      "Saved book 2600 to gutenberg_dataset/science_fiction/2600.txt\n",
      "Downloading book ID 135...\n",
      "Saved book 135 to gutenberg_dataset/science_fiction/135.txt\n",
      "Downloading book ID 120...\n",
      "Saved book 120 to gutenberg_dataset/science_fiction/120.txt\n",
      "Downloading book ID 1837...\n",
      "Saved book 1837 to gutenberg_dataset/science_fiction/1837.txt\n",
      "Downloading book ID 73...\n",
      "Saved book 73 to gutenberg_dataset/science_fiction/73.txt\n",
      "Loading Gutenberg metadata cache...\n",
      "Book 16328 already exists at gutenberg_dataset/poetry/16328.txt, skipping download.\n",
      "Book 1322 already exists at gutenberg_dataset/poetry/1322.txt, skipping download.\n",
      "Book 228 already exists at gutenberg_dataset/poetry/228.txt, skipping download.\n",
      "Book 2490 already exists at gutenberg_dataset/poetry/2490.txt, skipping download.\n",
      "Book 14568 already exists at gutenberg_dataset/poetry/14568.txt, skipping download.\n",
      "Book 9622 already exists at gutenberg_dataset/poetry/9622.txt, skipping download.\n",
      "Book 3333 already exists at gutenberg_dataset/poetry/3333.txt, skipping download.\n",
      "Book 1321 already exists at gutenberg_dataset/poetry/1321.txt, skipping download.\n",
      "Book 20 already exists at gutenberg_dataset/poetry/20.txt, skipping download.\n",
      "Book 847 already exists at gutenberg_dataset/poetry/847.txt, skipping download.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from gutenbergpy.textget import get_text_by_id\n",
    "from gutenbergpy.gutenbergcache import GutenbergCache\n",
    "\n",
    "\n",
    "# Define genres and corresponding Project Gutenberg bookshelf URLs\n",
    "bookshelves = {\n",
    "    'science_fiction': 'https://www.gutenberg.org/ebooks/bookshelf/41',\n",
    "    'poetry': 'https://www.gutenberg.org/ebooks/bookshelf/60',\n",
    "}\n",
    "\n",
    "# Function to extract book IDs from a Project Gutenberg bookshelf\n",
    "def get_book_ids_from_bookshelf(url, limit=10):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    book_links = soup.select('li.booklink a.link')\n",
    "    book_ids = []\n",
    "\n",
    "    for link in book_links:\n",
    "        href = link.get('href')\n",
    "        if href.startswith('/ebooks/'):\n",
    "            book_id = href.split('/')[-1]\n",
    "            if book_id.isdigit():\n",
    "                book_ids.append(int(book_id))\n",
    "                if len(book_ids) == limit:\n",
    "                    break\n",
    "    return book_ids\n",
    "\n",
    "# Function to download book texts given their IDs\n",
    "def download_books(book_ids, output_folder):\n",
    "    from gutenbergpy.textget import get_text_by_id\n",
    "    from gutenbergpy.gutenbergcache import GutenbergCache\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(\"Loading Gutenberg metadata cache...\")\n",
    "    cache = GutenbergCache.get_cache()\n",
    "    for book_id in book_ids:\n",
    "        output_path = os.path.join(output_folder, f\"{book_id}.txt\")\n",
    "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "            print(f\"Book {book_id} already exists at {output_path}, skipping download.\")\n",
    "            continue\n",
    "        print(f\"Downloading book ID {book_id}...\")\n",
    "        try:\n",
    "            text_bytes = get_text_by_id(book_id)\n",
    "            text_str = text_bytes.decode('utf-8', errors='ignore')\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text_str)\n",
    "            print(f\"Saved book {book_id} to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading book {book_id}: {e}\")\n",
    "\n",
    "# Utility function to download books by genre\n",
    "def download_books_to_dataset(bookshelf_url, genre, limit=10, base_folder=\"gutenberg_dataset\"):\n",
    "    output_folder = os.path.join(base_folder, genre)\n",
    "    book_ids = get_book_ids_from_bookshelf(bookshelf_url, limit=limit)\n",
    "    download_books(book_ids, output_folder=output_folder)\n",
    "\n",
    "# Download books for each genre\n",
    "for genre, url in bookshelves.items():\n",
    "    download_books_to_dataset(url, genre=genre, limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1261d4",
   "metadata": {
    "id": "cc1261d4"
   },
   "source": [
    "## ðŸ§¹ Data Cleaning and JSONL Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826ddea3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "826ddea3",
    "outputId": "af0d91b8-0476-4ea8-825e-f6b918356604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gutenberg_dataset.jsonl already exists and is non-empty, skipping processing.\n",
      "Dataset written to gutenberg_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Define input directories and output file\n",
    "INPUT_DIRS = {\n",
    "    \"science_fiction\": Path(\"gutenberg_dataset/science_fiction\"),\n",
    "    \"poetry\": Path(\"gutenberg_dataset/poetry\"),\n",
    "}\n",
    "OUTPUT_FILE = Path(\"gutenberg_dataset.jsonl\")\n",
    "\n",
    "# Regex patterns to remove Project Gutenberg headers/footers\n",
    "HEADER_PATTERN = re.compile(\n",
    "    r\"\\*{3}\\s*START OF THIS PROJECT GUTENBERG EBOOK.*?\\*{3}\", re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "FOOTER_PATTERN = re.compile(\n",
    "    r\"\\*{3}\\s*END OF THIS PROJECT GUTENBERG EBOOK.*\", re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "\n",
    "# Function to clean Gutenberg text\n",
    "def clean_gutenberg_text(text: str) -> str:\n",
    "    text = HEADER_PATTERN.sub(\"\", text)\n",
    "    text = FOOTER_PATTERN.sub(\"\", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Function to process and write data to JSONL\n",
    "def process_and_write_jsonl(input_dirs: dict, output_path: Path):\n",
    "    if output_path.exists() and output_path.stat().st_size > 0:\n",
    "        print(f\"{output_path} already exists and is non-empty, skipping processing.\")\n",
    "        return\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as out_file:\n",
    "        for source_label, folder in input_dirs.items():\n",
    "            txt_files = list(folder.rglob(\"*.txt\"))\n",
    "            for txt_path in tqdm(txt_files, desc=f\"Processing {source_label}\"):\n",
    "                try:\n",
    "                    raw = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                    clean = clean_gutenberg_text(raw)\n",
    "                    if not clean:\n",
    "                        continue\n",
    "                    record = {\n",
    "                        \"source\": source_label,\n",
    "                        \"filename\": txt_path.name,\n",
    "                        \"text\": clean,\n",
    "                    }\n",
    "                    out_file.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {txt_path}: {e}\")\n",
    "\n",
    "# Process and write the dataset\n",
    "os.makedirs(OUTPUT_FILE.parent, exist_ok=True)\n",
    "process_and_write_jsonl(INPUT_DIRS, OUTPUT_FILE)\n",
    "print(f\"Dataset written to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a42369",
   "metadata": {
    "id": "f7a42369"
   },
   "source": [
    "## ðŸ§  Model Fine-Tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb3d2ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283,
     "referenced_widgets": [
      "927cb5416d0a4623894b5becec63ff3c",
      "fc5ba1536ee94daa83c8d5538f41e0de",
      "c81899bf07584883ae460ba4449ec9a0",
      "056f27b35fcd45b5a7cb7b69da0451ca",
      "426e3e8965bf4018b4fcc0e28132b507",
      "1ba3aa23e48f447999d121dd2f0ac75a",
      "2253de7d8f8c48b9990800575039943e",
      "696ea7fbeba34cc78bb9573d47c50ace",
      "140a014f878447a0bc284699c02a9996",
      "448e63b12f154391b86b2d80b8dadfad",
      "02ad8180e8dd40f085b02282bdd91e91"
     ]
    },
    "id": "ceb3d2ce",
    "outputId": "71385b57-fef1-43ff-96c7-2970f4b078db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded with 20 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n",
      "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'mps', 'cuda', 'xpu', 'npu', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'hpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'mps', 'cuda', 'xpu', 'npu', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'hpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load model with 8-bit precision\u001b[39;00m\n\u001b[32m     20\u001b[39m bnb_config = BitsAndBytesConfig(load_in_8bit=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m model = prepare_model_for_kbit_training(model)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Configure LoRA\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4389\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4386\u001b[39m     hf_quantizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4389\u001b[39m     \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4393\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4395\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4396\u001b[39m     torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)\n\u001b[32m   4397\u001b[39m     device_map = hf_quantizer.update_device_map(device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:81\u001b[39m, in \u001b[36mBnb8BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[32m     80\u001b[39m bnb_multibackend_is_enabled = is_bitsandbytes_multi_backend_available()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[43mvalidate_bnb_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mfrom_tf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mfrom_flax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m sure the weights are in PyTorch format.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     87\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/integrations/bitsandbytes.py:560\u001b[39m, in \u001b[36mvalidate_bnb_backend_availability\u001b[39m\u001b[34m(raise_exception)\u001b[39m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_bnb_multi_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _validate_bnb_cuda_backend_availability(raise_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/integrations/bitsandbytes.py:517\u001b[39m, in \u001b[36m_validate_bnb_multi_backend_availability\u001b[39m\u001b[34m(raise_exception)\u001b[39m\n\u001b[32m    511\u001b[39m     err_msg = (\n\u001b[32m    512\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of the available devices `available_devices = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_devices\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbnb_supported_devices_with_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m     )\n\u001b[32m    516\u001b[39m     logger.error(err_msg)\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[32m    519\u001b[39m logger.warning(\u001b[33m\"\u001b[39m\u001b[33mNo supported devices found for bitsandbytes multi-backend.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: None of the available devices `available_devices = None` are supported by the bitsandbytes version you have installed: `bnb_supported_devices = {'mps', 'cuda', 'xpu', 'npu', '\"cpu\" (needs an Intel CPU and intel_extension_for_pytorch installed and compatible with the PyTorch version)', 'hpu'}`. Please check the docs to see if the backend you intend to use is available and how to install it: https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "dataset = Dataset.from_list(data)\n",
    "print(f\"Dataset loaded with {len(dataset)} records.\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "MODEL_NAME = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with 8-bit precision\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"filename\", \"source\"])\n",
    "\n",
    "# Create training arguments (SFTConfig wraps TrainingArguments internally)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./poetic_sci_fi_model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir=True,\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    tokenizer_name_or_path=MODEL_NAME  # ðŸ‘ˆ NEW: Required by latest SFTTrainer\n",
    ")\n",
    "\n",
    "# Initialize trainer (tokenizer no longer passed here)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model_path = \"./poetic_sci_fi_model\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02f4a9",
   "metadata": {
    "id": "1c02f4a9"
   },
   "source": [
    "## âœ¨ Story Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb9c25",
   "metadata": {
    "id": "3ffb9c25"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define a prompt\n",
    "prompt = \"In the twilight of the cosmos, a lone traveler discovers a hidden world where dreams manifest into reality.\"\n",
    "\n",
    "# Generate a story\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=1000,\n",
    "    do_sample=True,\n",
    "    temperature=0.95,\n",
    "    top_k=50,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=1.1,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Poetic Sci-Fi Story:\\n\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02ad8180e8dd40f085b02282bdd91e91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "056f27b35fcd45b5a7cb7b69da0451ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_448e63b12f154391b86b2d80b8dadfad",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_02ad8180e8dd40f085b02282bdd91e91",
      "value": "â€‡20/20â€‡[00:17&lt;00:00,â€‡â€‡1.12â€‡examples/s]"
     }
    },
    "140a014f878447a0bc284699c02a9996": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1ba3aa23e48f447999d121dd2f0ac75a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2253de7d8f8c48b9990800575039943e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "426e3e8965bf4018b4fcc0e28132b507": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "448e63b12f154391b86b2d80b8dadfad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "696ea7fbeba34cc78bb9573d47c50ace": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "927cb5416d0a4623894b5becec63ff3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc5ba1536ee94daa83c8d5538f41e0de",
       "IPY_MODEL_c81899bf07584883ae460ba4449ec9a0",
       "IPY_MODEL_056f27b35fcd45b5a7cb7b69da0451ca"
      ],
      "layout": "IPY_MODEL_426e3e8965bf4018b4fcc0e28132b507"
     }
    },
    "c81899bf07584883ae460ba4449ec9a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_696ea7fbeba34cc78bb9573d47c50ace",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_140a014f878447a0bc284699c02a9996",
      "value": 20
     }
    },
    "fc5ba1536ee94daa83c8d5538f41e0de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ba3aa23e48f447999d121dd2f0ac75a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2253de7d8f8c48b9990800575039943e",
      "value": "Map:â€‡100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
