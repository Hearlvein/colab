{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e78fb2",
   "metadata": {},
   "source": [
    "# 🪐 Fine-Tuning GPT-2 with LoRA for Poetic Sci-Fi/Fantasy Story Generation\n",
    "\n",
    "This notebook guides you through fine-tuning GPT-2 using Low-Rank Adaptation (LoRA) to generate poetic sci-fi/fantasy short stories. The process includes:\n",
    "- Dataset preparation from Project Gutenberg\n",
    "- Model fine-tuning with LoRA\n",
    "- Story generation based on user prompts\n",
    "\n",
    "**Note:** This notebook is designed for execution in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb754d",
   "metadata": {},
   "source": [
    "## 🔧 Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install -q transformers datasets peft trl bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b944d2",
   "metadata": {},
   "source": [
    "## 📚 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define genres and corresponding Project Gutenberg bookshelf URLs\n",
    "bookshelves = {\n",
    "    'science_fiction': 'https://www.gutenberg.org/ebooks/bookshelf/41',\n",
    "    'poetry': 'https://www.gutenberg.org/ebooks/bookshelf/60',\n",
    "}\n",
    "\n",
    "# Function to extract book IDs from a Project Gutenberg bookshelf\n",
    "def get_book_ids_from_bookshelf(url, limit=10):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    book_links = soup.select('li.booklink a.link')\n",
    "    book_ids = []\n",
    "\n",
    "    for link in book_links:\n",
    "        href = link.get('href')\n",
    "        if href.startswith('/ebooks/'):\n",
    "            book_id = href.split('/')[-1]\n",
    "            if book_id.isdigit():\n",
    "                book_ids.append(int(book_id))\n",
    "                if len(book_ids) == limit:\n",
    "                    break\n",
    "    return book_ids\n",
    "\n",
    "# Function to download book texts given their IDs\n",
    "def download_books(book_ids, output_folder):\n",
    "    from gutenbergpy.textget import get_text_by_id\n",
    "    from gutenbergpy.gutenbergcache import GutenbergCache\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(\"Loading Gutenberg metadata cache...\")\n",
    "    cache = GutenbergCache.get_cache()\n",
    "    for book_id in book_ids:\n",
    "        output_path = os.path.join(output_folder, f\"{book_id}.txt\")\n",
    "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "            print(f\"Book {book_id} already exists at {output_path}, skipping download.\")\n",
    "            continue\n",
    "        print(f\"Downloading book ID {book_id}...\")\n",
    "        try:\n",
    "            text_bytes = get_text_by_id(book_id)\n",
    "            text_str = text_bytes.decode('utf-8', errors='ignore')\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(text_str)\n",
    "            print(f\"Saved book {book_id} to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading book {book_id}: {e}\")\n",
    "\n",
    "# Utility function to download books by genre\n",
    "def download_books_to_dataset(bookshelf_url, genre, limit=10, base_folder=\"gutenberg_dataset\"):\n",
    "    output_folder = os.path.join(base_folder, genre)\n",
    "    book_ids = get_book_ids_from_bookshelf(bookshelf_url, limit=limit)\n",
    "    download_books(book_ids, output_folder=output_folder)\n",
    "\n",
    "# Download books for each genre\n",
    "for genre, url in bookshelves.items():\n",
    "    download_books_to_dataset(url, genre=genre, limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1261d4",
   "metadata": {},
   "source": [
    "## 🧹 Data Cleaning and JSONL Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ddea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input directories and output file\n",
    "INPUT_DIRS = {\n",
    "    \"science_fiction\": Path(\"gutenberg_dataset/science_fiction\"),\n",
    "    \"poetry\": Path(\"gutenberg_dataset/poetry\"),\n",
    "}\n",
    "OUTPUT_FILE = Path(\"gutenberg_dataset.jsonl\")\n",
    "\n",
    "# Regex patterns to remove Project Gutenberg headers/footers\n",
    "HEADER_PATTERN = re.compile(\n",
    "    r\"\\*{3}\\s*START OF THIS PROJECT GUTENBERG EBOOK.*?\\*{3}\", re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "FOOTER_PATTERN = re.compile(\n",
    "    r\"\\*{3}\\s*END OF THIS PROJECT GUTENBERG EBOOK.*\", re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "\n",
    "# Function to clean Gutenberg text\n",
    "def clean_gutenberg_text(text: str) -> str:\n",
    "    text = HEADER_PATTERN.sub(\"\", text)\n",
    "    text = FOOTER_PATTERN.sub(\"\", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Function to process and write data to JSONL\n",
    "def process_and_write_jsonl(input_dirs: dict, output_path: Path):\n",
    "    if output_path.exists() and output_path.stat().st_size > 0:\n",
    "        print(f\"{output_path} already exists and is non-empty, skipping processing.\")\n",
    "        return\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as out_file:\n",
    "        for source_label, folder in input_dirs.items():\n",
    "            txt_files = list(folder.rglob(\"*.txt\"))\n",
    "            for txt_path in tqdm(txt_files, desc=f\"Processing {source_label}\"):\n",
    "                try:\n",
    "                    raw = txt_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                    clean = clean_gutenberg_text(raw)\n",
    "                    if not clean:\n",
    "                        continue\n",
    "                    record = {\n",
    "                        \"source\": source_label,\n",
    "                        \"filename\": txt_path.name,\n",
    "                        \"text\": clean,\n",
    "                    }\n",
    "                    out_file.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {txt_path}: {e}\")\n",
    "\n",
    "# Process and write the dataset\n",
    "os.makedirs(OUTPUT_FILE.parent, exist_ok=True)\n",
    "process_and_write_jsonl(INPUT_DIRS, OUTPUT_FILE)\n",
    "print(f\"Dataset written to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a42369",
   "metadata": {},
   "source": [
    "## 🧠 Model Fine-Tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "dataset = Dataset.from_list(data)\n",
    "print(f\"Dataset loaded with {len(dataset)} records.\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "MODEL_NAME = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model with 8-bit precision\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"filename\", \"source\"])\n",
    "\n",
    "# Training configuration\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./poetic_sci_fi_model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model_path = \"./poetic_sci_fi_model\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02f4a9",
   "metadata": {},
   "source": [
    "## ✨ Story Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Define a prompt\n",
    "prompt = \"In the twilight of the cosmos, a lone traveler discovers a hidden world where dreams manifest into reality.\"\n",
    "\n",
    "# Generate a story\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=1000,\n",
    "    do_sample=True,\n",
    "    temperature=0.95,\n",
    "    top_k=50,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=1.1,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Poetic Sci-Fi Story:\\n\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
