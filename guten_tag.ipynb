{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84ee7c41",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 on Sci-Fi & Poetry with LoRA\n",
    "Adapting from: https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q datasets transformers accelerate peft bitsandbytes einops\n",
    "%pip install -q beautifulsoup4 requests gutenbergpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad321338",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from gutenbergpy.textget import get_text_by_id\n",
    "from gutenbergpy.gutenbergcache import GutenbergCache\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc728c95",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Utility: Extract book IDs from a Gutenberg bookshelf\n",
    "def get_book_ids_from_bookshelf(url, limit=10):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    book_links = soup.select(\"li.booklink a.link\")\n",
    "    book_ids = []\n",
    "    for link in book_links:\n",
    "        href = link.get(\"href\")\n",
    "        if href.startswith(\"/ebooks/\"):\n",
    "            book_id = href.split(\"/\")[-1]\n",
    "            if book_id.isdigit():\n",
    "                book_ids.append(int(book_id))\n",
    "                if len(book_ids) == limit:\n",
    "                    break\n",
    "    return book_ids\n",
    "\n",
    "# Download and cache books\n",
    "def download_books(book_ids, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(\"Loading Gutenberg metadata cache...\")\n",
    "    cache = GutenbergCache.get_cache()\n",
    "    for book_id in book_ids:\n",
    "        output_path = os.path.join(output_folder, f\"{book_id}.txt\")\n",
    "        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:\n",
    "            print(f\"Book {book_id} already exists. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"Downloading book ID {book_id}...\")\n",
    "            text_bytes = get_text_by_id(book_id)\n",
    "            text_str = text_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {book_id}: {e}\")\n",
    "\n",
    "def download_books_to_dataset(bookshelf_url, genre, limit=10, base_folder=\"gutenberg_dataset\"):\n",
    "    folder = os.path.join(base_folder, genre)\n",
    "    ids = get_book_ids_from_bookshelf(bookshelf_url, limit)\n",
    "    download_books(ids, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bookshelves\n",
    "bookshelves = {\n",
    "    \"fiction\": \"https://www.gutenberg.org/ebooks/bookshelf/480\",\n",
    "    \"poetry\": \"https://www.gutenberg.org/ebooks/bookshelf/60\",\n",
    "}\n",
    "\n",
    "# Download books by genre\n",
    "for genre, url in bookshelves.items():\n",
    "    download_books_to_dataset(url, genre, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ce66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and structure the dataset\n",
    "HEADER_PATTERN = re.compile(r\"\\*{3}\\s*START OF THIS PROJECT GUTENBERG EBOOK.*?\\*{3}\", re.IGNORECASE | re.DOTALL)\n",
    "FOOTER_PATTERN = re.compile(r\"\\*{3}\\s*END OF THIS PROJECT GUTENBERG EBOOK.*\", re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = HEADER_PATTERN.sub(\"\", text)\n",
    "    text = FOOTER_PATTERN.sub(\"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def build_jsonl_dataset(input_dirs, output_file):\n",
    "    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "        print(f\"{output_file} exists. Skipping creation.\")\n",
    "        return\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for genre, folder in input_dirs.items():\n",
    "            for path in Path(folder).rglob(\"*.txt\"):\n",
    "                try:\n",
    "                    raw = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "                    cleaned = clean_text(raw)\n",
    "                    if cleaned:\n",
    "                        json.dump({\"source\": genre, \"filename\": path.name, \"text\": cleaned}, out_f, ensure_ascii=False)\n",
    "                        out_f.write(\"\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "# Prepare dataset\n",
    "INPUT_DIRS = {\n",
    "    \"fiction\": \"gutenberg_dataset/fiction\",\n",
    "    \"poetry\": \"gutenberg_dataset/poetry\",\n",
    "}\n",
    "OUTPUT_FILE = \"gutenberg_dataset.jsonl\"\n",
    "build_jsonl_dataset(INPUT_DIRS, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e903dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from datasets import Dataset\n",
    "with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "dataset = Dataset.from_list(data)\n",
    "print(f\"Loaded {len(dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41af54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(example):\n",
    "    result = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\", \"filename\", \"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA setup\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea554ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training\n",
    "output_dir = \"./gpt2-lora-sci-fi-poetry\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=output_dir, tokenizer=output_dir)\n",
    "\n",
    "prompt = \"Beneath the rusted moons of Elarion, the last poet of Earth recited verses to the wind.\"\n",
    "\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.95,\n",
    "    top_k=50,\n",
    "    top_p=0.92,\n",
    "    repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(\"\\nGenerated Poetic Sci-Fi Story:\\n\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
